{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1_Learning rate\n",
    "-------------\n",
    "- learning rate를 10, 0.0001 등 너무 크거나 작은 값으로 조정해보며 문제점을 파악한다.\n",
    "- Big learning rate: overshooting\n",
    "- Small learning rate: many iterations until convergence, trapping in local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a3dec1f6b2d6>:48: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "0 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "1 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "2 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "3 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "4 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "5 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "6 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "7 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "8 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "9 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "10 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "11 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "12 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "13 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "14 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "15 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "16 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "17 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "18 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "19 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "20 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "21 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "22 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "23 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "24 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "25 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "26 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "27 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "28 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "29 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "30 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "31 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "32 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "33 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "34 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "35 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "36 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "37 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "38 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "39 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "40 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "41 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "42 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "43 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "44 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "45 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "46 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "47 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "48 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "49 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "50 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "51 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "52 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "53 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "54 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "55 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "56 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "57 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "58 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "59 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "60 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "61 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "62 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "63 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "64 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "65 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "66 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "67 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "68 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "69 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "70 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "71 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "72 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "73 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "74 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "75 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "76 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "77 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "78 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "79 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "80 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "81 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "82 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "83 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "84 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "85 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "86 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "87 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "88 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "89 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "90 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "91 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "92 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "93 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "94 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "95 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "96 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "97 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "98 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "99 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "100 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "101 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "102 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "103 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "104 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "105 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "106 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "107 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "108 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "109 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "110 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "111 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "112 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "113 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "115 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "116 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "117 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "118 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "119 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "120 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "121 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "122 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "123 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "124 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "125 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "126 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "127 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "128 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "129 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "130 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "131 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "132 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "133 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "134 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "135 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "136 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "137 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "138 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "139 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "140 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "141 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "142 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "143 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "144 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "145 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "146 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "147 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "148 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "149 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "150 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "151 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "152 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "153 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "154 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "155 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "156 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "157 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "158 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "159 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "160 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "161 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "162 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "163 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "164 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "165 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "166 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "167 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "168 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "169 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "170 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "171 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "172 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "173 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "174 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "175 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "176 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "177 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "178 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "179 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "180 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "181 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "182 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "183 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "184 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "185 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "186 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "187 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "188 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "189 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "190 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "191 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "192 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "193 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "194 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "195 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "196 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "197 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "198 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "199 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "200 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_Normalization\n",
    "-------------\n",
    "- normalization을 하지 않았을 경우 nan, Inf 등이 뜬다.\n",
    "- **MinMaxScaler**라는 normalization을 위한 함수를 정의해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  0.4993103 \n",
      "Prediction:\n",
      " [[1.3444979 ]\n",
      " [2.1943438 ]\n",
      " [1.4994034 ]\n",
      " [0.74916124]\n",
      " [1.1013528 ]\n",
      " [0.9656154 ]\n",
      " [0.4695969 ]\n",
      " [0.689042  ]]\n",
      "1 Cost:  0.4992786 \n",
      "Prediction:\n",
      " [[1.3444669 ]\n",
      " [2.1943111 ]\n",
      " [1.4993767 ]\n",
      " [0.7491411 ]\n",
      " [1.1013287 ]\n",
      " [0.96559215]\n",
      " [0.46958092]\n",
      " [0.6890255 ]]\n",
      "2 Cost:  0.49924698 \n",
      "Prediction:\n",
      " [[1.3444357 ]\n",
      " [2.1942785 ]\n",
      " [1.4993502 ]\n",
      " [0.7491209 ]\n",
      " [1.1013048 ]\n",
      " [0.9655687 ]\n",
      " [0.46956494]\n",
      " [0.6890091 ]]\n",
      "3 Cost:  0.4992153 \n",
      "Prediction:\n",
      " [[1.3444043 ]\n",
      " [2.1942463 ]\n",
      " [1.4993234 ]\n",
      " [0.74910074]\n",
      " [1.1012805 ]\n",
      " [0.9655455 ]\n",
      " [0.46954894]\n",
      " [0.6889927 ]]\n",
      "4 Cost:  0.4991838 \n",
      "Prediction:\n",
      " [[1.3443733 ]\n",
      " [2.194214  ]\n",
      " [1.4992967 ]\n",
      " [0.74908054]\n",
      " [1.1012566 ]\n",
      " [0.9655221 ]\n",
      " [0.46953297]\n",
      " [0.68897617]]\n",
      "5 Cost:  0.49915218 \n",
      "Prediction:\n",
      " [[1.3443421]\n",
      " [2.1941817]\n",
      " [1.49927  ]\n",
      " [0.7490604]\n",
      " [1.1012325]\n",
      " [0.9654988]\n",
      " [0.469517 ]\n",
      " [0.6889598]]\n",
      "6 Cost:  0.49912047 \n",
      "Prediction:\n",
      " [[1.3443109 ]\n",
      " [2.194149  ]\n",
      " [1.4992434 ]\n",
      " [0.7490402 ]\n",
      " [1.1012084 ]\n",
      " [0.9654755 ]\n",
      " [0.46950096]\n",
      " [0.6889433 ]]\n",
      "7 Cost:  0.49908888 \n",
      "Prediction:\n",
      " [[1.3442795 ]\n",
      " [2.1941168 ]\n",
      " [1.4992167 ]\n",
      " [0.74902   ]\n",
      " [1.1011842 ]\n",
      " [0.96545213]\n",
      " [0.469485  ]\n",
      " [0.6889269 ]]\n",
      "8 Cost:  0.49905732 \n",
      "Prediction:\n",
      " [[1.3442483 ]\n",
      " [2.1940844 ]\n",
      " [1.49919   ]\n",
      " [0.74899983]\n",
      " [1.1011604 ]\n",
      " [0.9654288 ]\n",
      " [0.46946904]\n",
      " [0.6889104 ]]\n",
      "9 Cost:  0.49902564 \n",
      "Prediction:\n",
      " [[1.3442173 ]\n",
      " [2.1940517 ]\n",
      " [1.4991634 ]\n",
      " [0.7489797 ]\n",
      " [1.1011363 ]\n",
      " [0.9654055 ]\n",
      " [0.46945304]\n",
      " [0.688894  ]]\n",
      "10 Cost:  0.498994 \n",
      "Prediction:\n",
      " [[1.3441861 ]\n",
      " [2.1940193 ]\n",
      " [1.4991368 ]\n",
      " [0.7489595 ]\n",
      " [1.1011121 ]\n",
      " [0.96538216]\n",
      " [0.46943706]\n",
      " [0.6888776 ]]\n",
      "11 Cost:  0.49896234 \n",
      "Prediction:\n",
      " [[1.3441547 ]\n",
      " [2.193987  ]\n",
      " [1.49911   ]\n",
      " [0.74893934]\n",
      " [1.101088  ]\n",
      " [0.9653589 ]\n",
      " [0.4694211 ]\n",
      " [0.68886113]]\n",
      "12 Cost:  0.49893075 \n",
      "Prediction:\n",
      " [[1.3441237 ]\n",
      " [2.1939545 ]\n",
      " [1.4990834 ]\n",
      " [0.74891907]\n",
      " [1.1010641 ]\n",
      " [0.9653355 ]\n",
      " [0.46940514]\n",
      " [0.6888446 ]]\n",
      "13 Cost:  0.4988992 \n",
      "Prediction:\n",
      " [[1.3440925 ]\n",
      " [2.193922  ]\n",
      " [1.4990567 ]\n",
      " [0.748899  ]\n",
      " [1.10104   ]\n",
      " [0.96531224]\n",
      " [0.46938914]\n",
      " [0.6888283 ]]\n",
      "14 Cost:  0.4988675 \n",
      "Prediction:\n",
      " [[1.3440613 ]\n",
      " [2.1938896 ]\n",
      " [1.49903   ]\n",
      " [0.7488787 ]\n",
      " [1.1010159 ]\n",
      " [0.9652888 ]\n",
      " [0.46937314]\n",
      " [0.6888118 ]]\n",
      "15 Cost:  0.49883592 \n",
      "Prediction:\n",
      " [[1.3440299 ]\n",
      " [2.1938572 ]\n",
      " [1.4990034 ]\n",
      " [0.74885863]\n",
      " [1.1009917 ]\n",
      " [0.9652656 ]\n",
      " [0.4693572 ]\n",
      " [0.6887953 ]]\n",
      "16 Cost:  0.4988044 \n",
      "Prediction:\n",
      " [[1.3439987 ]\n",
      " [2.193825  ]\n",
      " [1.4989767 ]\n",
      " [0.7488384 ]\n",
      " [1.1009678 ]\n",
      " [0.96524227]\n",
      " [0.4693412 ]\n",
      " [0.688779  ]]\n",
      "17 Cost:  0.4987727 \n",
      "Prediction:\n",
      " [[1.3439677 ]\n",
      " [2.1937923 ]\n",
      " [1.4989501 ]\n",
      " [0.7488183 ]\n",
      " [1.1009437 ]\n",
      " [0.9652189 ]\n",
      " [0.46932518]\n",
      " [0.6887625 ]]\n",
      "18 Cost:  0.4987411 \n",
      "Prediction:\n",
      " [[1.3439364 ]\n",
      " [2.19376   ]\n",
      " [1.4989234 ]\n",
      " [0.7487981 ]\n",
      " [1.1009197 ]\n",
      " [0.96519566]\n",
      " [0.46930924]\n",
      " [0.68874604]]\n",
      "19 Cost:  0.4987095 \n",
      "Prediction:\n",
      " [[1.3439052 ]\n",
      " [2.1937275 ]\n",
      " [1.4988968 ]\n",
      " [0.7487779 ]\n",
      " [1.1008956 ]\n",
      " [0.96517223]\n",
      " [0.46929324]\n",
      " [0.68872964]]\n",
      "20 Cost:  0.49867794 \n",
      "Prediction:\n",
      " [[1.3438742 ]\n",
      " [2.193695  ]\n",
      " [1.4988701 ]\n",
      " [0.7487577 ]\n",
      " [1.1008716 ]\n",
      " [0.965149  ]\n",
      " [0.46927726]\n",
      " [0.6887132 ]]\n",
      "21 Cost:  0.49864632 \n",
      "Prediction:\n",
      " [[1.3438429 ]\n",
      " [2.1936626 ]\n",
      " [1.4988434 ]\n",
      " [0.74873763]\n",
      " [1.1008475 ]\n",
      " [0.9651257 ]\n",
      " [0.4692613 ]\n",
      " [0.68869674]]\n",
      "22 Cost:  0.4986147 \n",
      "Prediction:\n",
      " [[1.3438116 ]\n",
      " [2.1936302 ]\n",
      " [1.4988167 ]\n",
      " [0.7487174 ]\n",
      " [1.1008235 ]\n",
      " [0.9651023 ]\n",
      " [0.46924534]\n",
      " [0.68868035]]\n",
      "23 Cost:  0.4985832 \n",
      "Prediction:\n",
      " [[1.3437804 ]\n",
      " [2.193598  ]\n",
      " [1.4987903 ]\n",
      " [0.7486973 ]\n",
      " [1.1007994 ]\n",
      " [0.965079  ]\n",
      " [0.46922934]\n",
      " [0.6886639 ]]\n",
      "24 Cost:  0.4985515 \n",
      "Prediction:\n",
      " [[1.3437492 ]\n",
      " [2.1935654 ]\n",
      " [1.4987636 ]\n",
      " [0.74867713]\n",
      " [1.1007754 ]\n",
      " [0.9650557 ]\n",
      " [0.4692134 ]\n",
      " [0.68864745]]\n",
      "25 Cost:  0.49852005 \n",
      "Prediction:\n",
      " [[1.3437182 ]\n",
      " [2.1935332 ]\n",
      " [1.4987369 ]\n",
      " [0.7486569 ]\n",
      " [1.1007514 ]\n",
      " [0.9650324 ]\n",
      " [0.4691974 ]\n",
      " [0.68863106]]\n",
      "26 Cost:  0.49848837 \n",
      "Prediction:\n",
      " [[1.3436868 ]\n",
      " [2.1935008 ]\n",
      " [1.49871   ]\n",
      " [0.7486368 ]\n",
      " [1.1007272 ]\n",
      " [0.9650091 ]\n",
      " [0.46918142]\n",
      " [0.6886146 ]]\n",
      "27 Cost:  0.49845684 \n",
      "Prediction:\n",
      " [[1.3436556 ]\n",
      " [2.1934683 ]\n",
      " [1.4986836 ]\n",
      " [0.74861664]\n",
      " [1.1007031 ]\n",
      " [0.9649858 ]\n",
      " [0.46916544]\n",
      " [0.68859816]]\n",
      "28 Cost:  0.49842525 \n",
      "Prediction:\n",
      " [[1.3436246 ]\n",
      " [2.193436  ]\n",
      " [1.4986569 ]\n",
      " [0.7485964 ]\n",
      " [1.100679  ]\n",
      " [0.96496236]\n",
      " [0.4691495 ]\n",
      " [0.68858176]]\n",
      "29 Cost:  0.4983936 \n",
      "Prediction:\n",
      " [[1.3435934]\n",
      " [2.1934032]\n",
      " [1.4986304]\n",
      " [0.7485763]\n",
      " [1.100655 ]\n",
      " [0.9649391]\n",
      " [0.4691335]\n",
      " [0.6885653]]\n",
      "30 Cost:  0.49836203 \n",
      "Prediction:\n",
      " [[1.3435621 ]\n",
      " [2.1933708 ]\n",
      " [1.4986037 ]\n",
      " [0.7485561 ]\n",
      " [1.1006311 ]\n",
      " [0.9649158 ]\n",
      " [0.46911752]\n",
      " [0.68854886]]\n",
      "31 Cost:  0.4983304 \n",
      "Prediction:\n",
      " [[1.3435309 ]\n",
      " [2.1933384 ]\n",
      " [1.498577  ]\n",
      " [0.74853593]\n",
      " [1.100607  ]\n",
      " [0.96489245]\n",
      " [0.46910158]\n",
      " [0.6885325 ]]\n",
      "32 Cost:  0.49829882 \n",
      "Prediction:\n",
      " [[1.3434997]\n",
      " [2.193306 ]\n",
      " [1.4985503]\n",
      " [0.7485157]\n",
      " [1.100583 ]\n",
      " [0.9648692]\n",
      " [0.4690856]\n",
      " [0.688516 ]]\n",
      "33 Cost:  0.49826723 \n",
      "Prediction:\n",
      " [[1.3434685 ]\n",
      " [2.1932735 ]\n",
      " [1.4985236 ]\n",
      " [0.74849564]\n",
      " [1.1005588 ]\n",
      " [0.96484584]\n",
      " [0.4690696 ]\n",
      " [0.6884996 ]]\n",
      "34 Cost:  0.4982357 \n",
      "Prediction:\n",
      " [[1.3434373 ]\n",
      " [2.1932411 ]\n",
      " [1.498497  ]\n",
      " [0.7484755 ]\n",
      " [1.1005348 ]\n",
      " [0.9648226 ]\n",
      " [0.46905366]\n",
      " [0.68848324]]\n",
      "35 Cost:  0.4982041 \n",
      "Prediction:\n",
      " [[1.3434061]\n",
      " [2.1932087]\n",
      " [1.4984704]\n",
      " [0.7484553]\n",
      " [1.1005108]\n",
      " [0.9647992]\n",
      " [0.4690377]\n",
      " [0.6884667]]\n",
      "36 Cost:  0.49817255 \n",
      "Prediction:\n",
      " [[1.3433751 ]\n",
      " [2.1931763 ]\n",
      " [1.4984438 ]\n",
      " [0.74843514]\n",
      " [1.1004868 ]\n",
      " [0.9647758 ]\n",
      " [0.4690217 ]\n",
      " [0.68845034]]\n",
      "37 Cost:  0.49814105 \n",
      "Prediction:\n",
      " [[1.3433439 ]\n",
      " [2.193144  ]\n",
      " [1.4984171 ]\n",
      " [0.748415  ]\n",
      " [1.1004627 ]\n",
      " [0.96475255]\n",
      " [0.46900573]\n",
      " [0.68843395]]\n",
      "38 Cost:  0.49810937 \n",
      "Prediction:\n",
      " [[1.3433126 ]\n",
      " [2.1931114 ]\n",
      " [1.4983904 ]\n",
      " [0.74839485]\n",
      " [1.1004386 ]\n",
      " [0.9647292 ]\n",
      " [0.46898976]\n",
      " [0.6884175 ]]\n",
      "39 Cost:  0.49807787 \n",
      "Prediction:\n",
      " [[1.3432814 ]\n",
      " [2.1930792 ]\n",
      " [1.4983637 ]\n",
      " [0.74837464]\n",
      " [1.1004145 ]\n",
      " [0.964706  ]\n",
      " [0.46897385]\n",
      " [0.68840104]]\n",
      "40 Cost:  0.49804625 \n",
      "Prediction:\n",
      " [[1.3432502 ]\n",
      " [2.1930466 ]\n",
      " [1.4983371 ]\n",
      " [0.7483545 ]\n",
      " [1.1003904 ]\n",
      " [0.96468264]\n",
      " [0.4689578 ]\n",
      " [0.68838465]]\n",
      "41 Cost:  0.49801475 \n",
      "Prediction:\n",
      " [[1.343219  ]\n",
      " [2.1930144 ]\n",
      " [1.4983104 ]\n",
      " [0.7483344 ]\n",
      " [1.1003664 ]\n",
      " [0.9646594 ]\n",
      " [0.46894187]\n",
      " [0.6883682 ]]\n",
      "42 Cost:  0.4979832 \n",
      "Prediction:\n",
      " [[1.3431878 ]\n",
      " [2.192982  ]\n",
      " [1.4982839 ]\n",
      " [0.7483142 ]\n",
      " [1.1003424 ]\n",
      " [0.96463597]\n",
      " [0.4689259 ]\n",
      " [0.6883518 ]]\n",
      "43 Cost:  0.49795157 \n",
      "Prediction:\n",
      " [[1.3431566 ]\n",
      " [2.1929495 ]\n",
      " [1.4982572 ]\n",
      " [0.74829406]\n",
      " [1.1003183 ]\n",
      " [0.9646126 ]\n",
      " [0.46890995]\n",
      " [0.68833536]]\n",
      "44 Cost:  0.49792004 \n",
      "Prediction:\n",
      " [[1.3431256 ]\n",
      " [2.192917  ]\n",
      " [1.4982306 ]\n",
      " [0.7482739 ]\n",
      " [1.1002942 ]\n",
      " [0.96458936]\n",
      " [0.46889398]\n",
      " [0.6883189 ]]\n",
      "45 Cost:  0.49788848 \n",
      "Prediction:\n",
      " [[1.3430943 ]\n",
      " [2.1928847 ]\n",
      " [1.4982039 ]\n",
      " [0.74825376]\n",
      " [1.1002703 ]\n",
      " [0.964566  ]\n",
      " [0.468878  ]\n",
      " [0.6883025 ]]\n",
      "46 Cost:  0.49785694 \n",
      "Prediction:\n",
      " [[1.3430631 ]\n",
      " [2.1928523 ]\n",
      " [1.4981773 ]\n",
      " [0.74823356]\n",
      " [1.1002463 ]\n",
      " [0.96454275]\n",
      " [0.46886203]\n",
      " [0.6882861 ]]\n",
      "47 Cost:  0.49782532 \n",
      "Prediction:\n",
      " [[1.3430319 ]\n",
      " [2.1928196 ]\n",
      " [1.4981507 ]\n",
      " [0.7482134 ]\n",
      " [1.1002222 ]\n",
      " [0.96451944]\n",
      " [0.46884608]\n",
      " [0.6882697 ]]\n",
      "48 Cost:  0.4977938 \n",
      "Prediction:\n",
      " [[1.3430007 ]\n",
      " [2.1927874 ]\n",
      " [1.498124  ]\n",
      " [0.74819326]\n",
      " [1.100198  ]\n",
      " [0.96449614]\n",
      " [0.46883008]\n",
      " [0.6882533 ]]\n",
      "49 Cost:  0.4977623 \n",
      "Prediction:\n",
      " [[1.3429697 ]\n",
      " [2.192755  ]\n",
      " [1.4980974 ]\n",
      " [0.74817306]\n",
      " [1.1001741 ]\n",
      " [0.96447283]\n",
      " [0.46881416]\n",
      " [0.68823683]]\n",
      "50 Cost:  0.49773073 \n",
      "Prediction:\n",
      " [[1.3429384 ]\n",
      " [2.1927226 ]\n",
      " [1.4980708 ]\n",
      " [0.7481529 ]\n",
      " [1.10015   ]\n",
      " [0.96444947]\n",
      " [0.4687982 ]\n",
      " [0.6882204 ]]\n",
      "51 Cost:  0.49769917 \n",
      "Prediction:\n",
      " [[1.3429072 ]\n",
      " [2.1926901 ]\n",
      " [1.4980441 ]\n",
      " [0.74813277]\n",
      " [1.100126  ]\n",
      " [0.9644262 ]\n",
      " [0.46878222]\n",
      " [0.688204  ]]\n",
      "52 Cost:  0.49766764 \n",
      "Prediction:\n",
      " [[1.3428762 ]\n",
      " [2.1926577 ]\n",
      " [1.4980174 ]\n",
      " [0.7481126 ]\n",
      " [1.100102  ]\n",
      " [0.9644029 ]\n",
      " [0.46876624]\n",
      " [0.6881876 ]]\n",
      "53 Cost:  0.49763605 \n",
      "Prediction:\n",
      " [[1.3428448 ]\n",
      " [2.1926253 ]\n",
      " [1.4979907 ]\n",
      " [0.7480925 ]\n",
      " [1.1000779 ]\n",
      " [0.9643796 ]\n",
      " [0.4687503 ]\n",
      " [0.68817115]]\n",
      "54 Cost:  0.4976045 \n",
      "Prediction:\n",
      " [[1.3428136 ]\n",
      " [2.1925929 ]\n",
      " [1.4979641 ]\n",
      " [0.7480723 ]\n",
      " [1.1000537 ]\n",
      " [0.9643563 ]\n",
      " [0.46873435]\n",
      " [0.68815476]]\n",
      "55 Cost:  0.49757296 \n",
      "Prediction:\n",
      " [[1.3427824 ]\n",
      " [2.1925604 ]\n",
      " [1.4979374 ]\n",
      " [0.74805224]\n",
      " [1.1000298 ]\n",
      " [0.964333  ]\n",
      " [0.46871838]\n",
      " [0.68813837]]\n",
      "56 Cost:  0.49754143 \n",
      "Prediction:\n",
      " [[1.3427511 ]\n",
      " [2.192528  ]\n",
      " [1.497911  ]\n",
      " [0.74803203]\n",
      " [1.1000057 ]\n",
      " [0.9643097 ]\n",
      " [0.46870238]\n",
      " [0.688122  ]]\n",
      "57 Cost:  0.49750987 \n",
      "Prediction:\n",
      " [[1.3427202 ]\n",
      " [2.1924956 ]\n",
      " [1.4978843 ]\n",
      " [0.74801195]\n",
      " [1.0999817 ]\n",
      " [0.9642863 ]\n",
      " [0.46868643]\n",
      " [0.68810546]]\n",
      "58 Cost:  0.4974783 \n",
      "Prediction:\n",
      " [[1.3426889 ]\n",
      " [2.1924632 ]\n",
      " [1.4978576 ]\n",
      " [0.74799174]\n",
      " [1.0999576 ]\n",
      " [0.96426296]\n",
      " [0.46867052]\n",
      " [0.6880891 ]]\n",
      "59 Cost:  0.4974468 \n",
      "Prediction:\n",
      " [[1.3426577 ]\n",
      " [2.1924307 ]\n",
      " [1.4978309 ]\n",
      " [0.7479716 ]\n",
      " [1.0999337 ]\n",
      " [0.96423966]\n",
      " [0.46865457]\n",
      " [0.6880727 ]]\n",
      "60 Cost:  0.4974153 \n",
      "Prediction:\n",
      " [[1.3426267 ]\n",
      " [2.1923985 ]\n",
      " [1.4978042 ]\n",
      " [0.74795145]\n",
      " [1.0999095 ]\n",
      " [0.9642165 ]\n",
      " [0.46863857]\n",
      " [0.68805623]]\n",
      "61 Cost:  0.49738377 \n",
      "Prediction:\n",
      " [[1.3425955 ]\n",
      " [2.192366  ]\n",
      " [1.4977777 ]\n",
      " [0.7479313 ]\n",
      " [1.0998856 ]\n",
      " [0.96419317]\n",
      " [0.4686226 ]\n",
      " [0.68803984]]\n",
      "62 Cost:  0.49735227 \n",
      "Prediction:\n",
      " [[1.3425642 ]\n",
      " [2.1923337 ]\n",
      " [1.497751  ]\n",
      " [0.74791116]\n",
      " [1.0998615 ]\n",
      " [0.96416986]\n",
      " [0.46860665]\n",
      " [0.68802345]]\n",
      "63 Cost:  0.49732077 \n",
      "Prediction:\n",
      " [[1.342533  ]\n",
      " [2.1923013 ]\n",
      " [1.4977245 ]\n",
      " [0.747891  ]\n",
      " [1.0998375 ]\n",
      " [0.9641466 ]\n",
      " [0.46859068]\n",
      " [0.688007  ]]\n",
      "64 Cost:  0.4972893 \n",
      "Prediction:\n",
      " [[1.3425018 ]\n",
      " [2.192269  ]\n",
      " [1.4976978 ]\n",
      " [0.7478708 ]\n",
      " [1.0998135 ]\n",
      " [0.9641232 ]\n",
      " [0.46857473]\n",
      " [0.6879906 ]]\n",
      "65 Cost:  0.49725768 \n",
      "Prediction:\n",
      " [[1.3424708 ]\n",
      " [2.1922364 ]\n",
      " [1.4976711 ]\n",
      " [0.74785066]\n",
      " [1.0997894 ]\n",
      " [0.9640999 ]\n",
      " [0.4685588 ]\n",
      " [0.6879742 ]]\n",
      "66 Cost:  0.49722612 \n",
      "Prediction:\n",
      " [[1.3424395 ]\n",
      " [2.192204  ]\n",
      " [1.4976444 ]\n",
      " [0.74783057]\n",
      " [1.0997653 ]\n",
      " [0.9640765 ]\n",
      " [0.46854284]\n",
      " [0.68795776]]\n",
      "67 Cost:  0.49719462 \n",
      "Prediction:\n",
      " [[1.3424083 ]\n",
      " [2.1921716 ]\n",
      " [1.4976178 ]\n",
      " [0.7478104 ]\n",
      " [1.0997413 ]\n",
      " [0.9640533 ]\n",
      " [0.46852684]\n",
      " [0.6879414 ]]\n",
      "68 Cost:  0.4971632 \n",
      "Prediction:\n",
      " [[1.3423773 ]\n",
      " [2.1921394 ]\n",
      " [1.4975913 ]\n",
      " [0.7477902 ]\n",
      " [1.0997173 ]\n",
      " [0.96402997]\n",
      " [0.46851093]\n",
      " [0.687925  ]]\n",
      "69 Cost:  0.49713168 \n",
      "Prediction:\n",
      " [[1.3423461 ]\n",
      " [2.192107  ]\n",
      " [1.4975647 ]\n",
      " [0.74777013]\n",
      " [1.0996933 ]\n",
      " [0.96400666]\n",
      " [0.46849498]\n",
      " [0.68790853]]\n",
      "70 Cost:  0.49710009 \n",
      "Prediction:\n",
      " [[1.3423148 ]\n",
      " [2.1920743 ]\n",
      " [1.4975381 ]\n",
      " [0.74775   ]\n",
      " [1.0996692 ]\n",
      " [0.9639834 ]\n",
      " [0.468479  ]\n",
      " [0.68789214]]\n",
      "71 Cost:  0.4970686 \n",
      "Prediction:\n",
      " [[1.3422836 ]\n",
      " [2.192042  ]\n",
      " [1.4975114 ]\n",
      " [0.74772984]\n",
      " [1.0996451 ]\n",
      " [0.96396005]\n",
      " [0.46846306]\n",
      " [0.68787575]]\n",
      "72 Cost:  0.4970371 \n",
      "Prediction:\n",
      " [[1.3422524]\n",
      " [2.1920097]\n",
      " [1.4974847]\n",
      " [0.7477097]\n",
      " [1.0996212]\n",
      " [0.9639368]\n",
      " [0.4684471]\n",
      " [0.6878593]]\n",
      "73 Cost:  0.49700564 \n",
      "Prediction:\n",
      " [[1.3422214 ]\n",
      " [2.1919773 ]\n",
      " [1.4974582 ]\n",
      " [0.74768955]\n",
      " [1.0995972 ]\n",
      " [0.9639135 ]\n",
      " [0.46843114]\n",
      " [0.6878429 ]]\n",
      "74 Cost:  0.49697408 \n",
      "Prediction:\n",
      " [[1.3421901]\n",
      " [2.1919448]\n",
      " [1.4974314]\n",
      " [0.7476694]\n",
      " [1.0995731]\n",
      " [0.9638902]\n",
      " [0.4684152]\n",
      " [0.6878265]]\n",
      "75 Cost:  0.49694258 \n",
      "Prediction:\n",
      " [[1.3421589 ]\n",
      " [2.1919124 ]\n",
      " [1.4974049 ]\n",
      " [0.74764925]\n",
      " [1.099549  ]\n",
      " [0.96386695]\n",
      " [0.46839926]\n",
      " [0.68781006]]\n",
      "76 Cost:  0.49691108 \n",
      "Prediction:\n",
      " [[1.3421279 ]\n",
      " [2.19188   ]\n",
      " [1.4973782 ]\n",
      " [0.7476291 ]\n",
      " [1.099525  ]\n",
      " [0.9638436 ]\n",
      " [0.46838334]\n",
      " [0.6877937 ]]\n",
      "77 Cost:  0.49687958 \n",
      "Prediction:\n",
      " [[1.3420967 ]\n",
      " [2.1918476 ]\n",
      " [1.4973516 ]\n",
      " [0.7476089 ]\n",
      " [1.0995009 ]\n",
      " [0.96382034]\n",
      " [0.46836737]\n",
      " [0.6877773 ]]\n",
      "78 Cost:  0.49684802 \n",
      "Prediction:\n",
      " [[1.3420655 ]\n",
      " [2.1918151 ]\n",
      " [1.497325  ]\n",
      " [0.7475889 ]\n",
      " [1.0994768 ]\n",
      " [0.9637971 ]\n",
      " [0.46835142]\n",
      " [0.6877609 ]]\n",
      "79 Cost:  0.49681664 \n",
      "Prediction:\n",
      " [[1.3420342 ]\n",
      " [2.191783  ]\n",
      " [1.4972982 ]\n",
      " [0.74756867]\n",
      " [1.0994529 ]\n",
      " [0.9637738 ]\n",
      " [0.46833545]\n",
      " [0.68774456]]\n",
      "80 Cost:  0.49678513 \n",
      "Prediction:\n",
      " [[1.342003  ]\n",
      " [2.1917505 ]\n",
      " [1.4972717 ]\n",
      " [0.74754864]\n",
      " [1.0994289 ]\n",
      " [0.9637505 ]\n",
      " [0.46831948]\n",
      " [0.6877281 ]]\n",
      "81 Cost:  0.4967535 \n",
      "Prediction:\n",
      " [[1.341972  ]\n",
      " [2.1917179 ]\n",
      " [1.4972451 ]\n",
      " [0.7475284 ]\n",
      " [1.0994048 ]\n",
      " [0.9637272 ]\n",
      " [0.46830356]\n",
      " [0.68771166]]\n",
      "82 Cost:  0.49672213 \n",
      "Prediction:\n",
      " [[1.3419408 ]\n",
      " [2.1916857 ]\n",
      " [1.4972184 ]\n",
      " [0.7475082 ]\n",
      " [1.0993807 ]\n",
      " [0.96370393]\n",
      " [0.46828765]\n",
      " [0.6876953 ]]\n",
      "83 Cost:  0.49669066 \n",
      "Prediction:\n",
      " [[1.3419096 ]\n",
      " [2.1916533 ]\n",
      " [1.4971919 ]\n",
      " [0.74748814]\n",
      " [1.0993568 ]\n",
      " [0.96368057]\n",
      " [0.46827164]\n",
      " [0.6876789 ]]\n",
      "84 Cost:  0.4966593 \n",
      "Prediction:\n",
      " [[1.3418787 ]\n",
      " [2.191621  ]\n",
      " [1.4971653 ]\n",
      " [0.74746805]\n",
      " [1.0993329 ]\n",
      " [0.9636573 ]\n",
      " [0.4682557 ]\n",
      " [0.6876624 ]]\n",
      "85 Cost:  0.49662766 \n",
      "Prediction:\n",
      " [[1.3418474]\n",
      " [2.1915884]\n",
      " [1.4971386]\n",
      " [0.7474478]\n",
      " [1.0993088]\n",
      " [0.963634 ]\n",
      " [0.4682398]\n",
      " [0.6876461]]\n",
      "86 Cost:  0.49659616 \n",
      "Prediction:\n",
      " [[1.3418162 ]\n",
      " [2.191556  ]\n",
      " [1.4971119 ]\n",
      " [0.7474277 ]\n",
      " [1.0992848 ]\n",
      " [0.9636107 ]\n",
      " [0.46822384]\n",
      " [0.68762964]]\n",
      "87 Cost:  0.49656475 \n",
      "Prediction:\n",
      " [[1.341785  ]\n",
      " [2.1915238 ]\n",
      " [1.4970855 ]\n",
      " [0.74740756]\n",
      " [1.0992607 ]\n",
      " [0.9635875 ]\n",
      " [0.46820787]\n",
      " [0.68761325]]\n",
      "88 Cost:  0.49653327 \n",
      "Prediction:\n",
      " [[1.3417537 ]\n",
      " [2.1914914 ]\n",
      " [1.4970587 ]\n",
      " [0.74738747]\n",
      " [1.0992366 ]\n",
      " [0.96356416]\n",
      " [0.46819195]\n",
      " [0.6875969 ]]\n",
      "89 Cost:  0.49650168 \n",
      "Prediction:\n",
      " [[1.3417227 ]\n",
      " [2.1914587 ]\n",
      " [1.497032  ]\n",
      " [0.74736726]\n",
      " [1.0992126 ]\n",
      " [0.9635409 ]\n",
      " [0.468176  ]\n",
      " [0.68758047]]\n",
      "90 Cost:  0.49647036 \n",
      "Prediction:\n",
      " [[1.3416915 ]\n",
      " [2.1914268 ]\n",
      " [1.4970056 ]\n",
      " [0.7473472 ]\n",
      " [1.0991886 ]\n",
      " [0.9635176 ]\n",
      " [0.46816003]\n",
      " [0.687564  ]]\n",
      "91 Cost:  0.49643883 \n",
      "Prediction:\n",
      " [[1.3416603 ]\n",
      " [2.1913943 ]\n",
      " [1.4969789 ]\n",
      " [0.74732697]\n",
      " [1.0991645 ]\n",
      " [0.9634943 ]\n",
      " [0.46814412]\n",
      " [0.6875477 ]]\n",
      "92 Cost:  0.49640733 \n",
      "Prediction:\n",
      " [[1.3416293 ]\n",
      " [2.191362  ]\n",
      " [1.4969522 ]\n",
      " [0.74730694]\n",
      " [1.0991404 ]\n",
      " [0.96347094]\n",
      " [0.46812817]\n",
      " [0.68753123]]\n",
      "93 Cost:  0.49637586 \n",
      "Prediction:\n",
      " [[1.341598  ]\n",
      " [2.1913295 ]\n",
      " [1.4969255 ]\n",
      " [0.7472868 ]\n",
      " [1.0991166 ]\n",
      " [0.96344775]\n",
      " [0.4681122 ]\n",
      " [0.68751484]]\n",
      "94 Cost:  0.49634445 \n",
      "Prediction:\n",
      " [[1.3415668 ]\n",
      " [2.191297  ]\n",
      " [1.496899  ]\n",
      " [0.7472667 ]\n",
      " [1.0990925 ]\n",
      " [0.96342444]\n",
      " [0.4680963 ]\n",
      " [0.6874985 ]]\n",
      "95 Cost:  0.49631315 \n",
      "Prediction:\n",
      " [[1.3415359 ]\n",
      " [2.1912649 ]\n",
      " [1.4968727 ]\n",
      " [0.7472466 ]\n",
      " [1.0990686 ]\n",
      " [0.96340126]\n",
      " [0.4680804 ]\n",
      " [0.68748206]]\n",
      "96 Cost:  0.4962818 \n",
      "Prediction:\n",
      " [[1.3415049 ]\n",
      " [2.1912327 ]\n",
      " [1.4968461 ]\n",
      " [0.74722654]\n",
      " [1.0990447 ]\n",
      " [0.963378  ]\n",
      " [0.4680645 ]\n",
      " [0.6874656 ]]\n",
      "97 Cost:  0.4962504 \n",
      "Prediction:\n",
      " [[1.3414739 ]\n",
      " [2.1912003 ]\n",
      " [1.4968195 ]\n",
      " [0.7472065 ]\n",
      " [1.0990207 ]\n",
      " [0.9633548 ]\n",
      " [0.46804857]\n",
      " [0.68744934]]\n",
      "98 Cost:  0.496219 \n",
      "Prediction:\n",
      " [[1.341443  ]\n",
      " [2.191168  ]\n",
      " [1.4967929 ]\n",
      " [0.7471864 ]\n",
      " [1.0989966 ]\n",
      " [0.9633316 ]\n",
      " [0.46803263]\n",
      " [0.6874329 ]]\n",
      "99 Cost:  0.49618754 \n",
      "Prediction:\n",
      " [[1.3414117 ]\n",
      " [2.1911356 ]\n",
      " [1.4967663 ]\n",
      " [0.7471663 ]\n",
      " [1.0989728 ]\n",
      " [0.96330833]\n",
      " [0.46801674]\n",
      " [0.68741655]]\n",
      "100 Cost:  0.49615628 \n",
      "Prediction:\n",
      " [[1.3413807 ]\n",
      " [2.1911035 ]\n",
      " [1.4967399 ]\n",
      " [0.7471462 ]\n",
      " [1.098949  ]\n",
      " [0.9632852 ]\n",
      " [0.46800086]\n",
      " [0.6874002 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3_MNIST data\n",
    "-------------\n",
    "- 미국 우체국에서 우편번호를 쓴 숫자를 판별하기 위해 만든 데이터\n",
    "- X: 각 이미지는 28 * 28 = 784 픽셀로 이루어져 있다.\n",
    "- Y: 0~9까지의 숫자, 즉 label은 nb_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Flow:\n",
    "**from** tensorflow.examples.tutorials.mnist **import** input_data\n",
    "\n",
    "\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    ...\n",
    "\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "\n",
    "    ...\n",
    "\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "___\n",
    "\n",
    "-> mnist data에서, 100개씩의 batch를 불러와 학습한 후 (train), evaluation 단계에서 test data 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the neural network terminology:\n",
    "\n",
    "- one **epoch** = one forward pass and one backward pass of all the training examples\n",
    "- **batch size** = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "- number of iterations = number of passes, each pass using [batch size] number of examples.\n",
    "\n",
    "\n",
    "- To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "- Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 3.155379388\n",
      "Epoch: 0002 cost = 1.080789446\n",
      "Epoch: 0003 cost = 0.866853728\n",
      "Epoch: 0004 cost = 0.762582950\n",
      "Epoch: 0005 cost = 0.695226641\n",
      "Epoch: 0006 cost = 0.648443077\n",
      "Epoch: 0007 cost = 0.612284152\n",
      "Epoch: 0008 cost = 0.583516143\n",
      "Epoch: 0009 cost = 0.559341383\n",
      "Epoch: 0010 cost = 0.539036504\n",
      "Epoch: 0011 cost = 0.521877485\n",
      "Epoch: 0012 cost = 0.506519293\n",
      "Epoch: 0013 cost = 0.493281681\n",
      "Epoch: 0014 cost = 0.481551770\n",
      "Epoch: 0015 cost = 0.470288259\n",
      "Learning finished\n",
      "Accuracy:  0.8889\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size) # 55000 / 100\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
